{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features,labels,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7241379310344828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print \"accuracy:\", accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many POIs are predicted for the test set for your POI identifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted_poi = (pred == 1).sum()\n",
    "predicted_non_poi = (pred == 0).sum()\n",
    "print predicted_poi,predicted_non_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many people total are in your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "total_people = len(features_test)\n",
    "print total_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "print (predicted_non_poi*1.0)/total_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding TP,FP,TN,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class validate:\n",
    "    def __init__(self,true,pred):\n",
    "        self.true = true\n",
    "        self.pred = pred\n",
    "        self.tp=self.fp=self.tn=self.fn=0\n",
    "        for self.actual,self.predicted in zip(self.true,self.pred):\n",
    "            #print self.actual,self.predicted\n",
    "            if self.actual==1 and self.predicted==1:\n",
    "                self.tp+=1\n",
    "            elif self.actual==0 and self.predicted==1:\n",
    "                self.fp+=1\n",
    "            elif self.actual==0 and self.predicted==0:\n",
    "                self.tn+=1\n",
    "            elif self.actual==1 and self.predicted==0:\n",
    "                self.fn+=1\n",
    "    def find_tp_fp_tn_fn(self):\n",
    "        return [self.tp,self.fp,self.tn,self.fn]\n",
    "    def calc_precision(self):\n",
    "        self.precision = self.tp*1.0/(self.tp+self.fp)\n",
    "        return self.precision\n",
    "    def calc_recall(self):\n",
    "        self.recall = self.tp*1.0/(self.tp+self.fn)\n",
    "        return self.recall\n",
    "    def calc_fscore(self):\n",
    "        self.fscore = (2.0*self.precision*self.recall)/self.precision+self.recall\n",
    "        return self.fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "actual = np.array(labels_test).astype(int)\n",
    "pred = pred.astype(int)\n",
    "v = validate(actual,pred)\n",
    "tp = v.find_tp_fp_tn_fn()[0]\n",
    "print tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What’s the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "precision = v.calc_precision()\n",
    "print precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What’s the recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "recall = v.calc_recall()\n",
    "print recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  6\n",
      "FP:  3\n",
      "TN:  9\n",
      "FN:  2\n",
      "Precision:  0.666666666667\n",
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "pred1   = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "actual1 = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "v1 = validate(actual1,pred1)\n",
    "tp,fp,tn,fn = v1.find_tp_fp_tn_fn()\n",
    "precision = v1.calc_precision()\n",
    "recall = v1.calc_recall()\n",
    "print \"TP: \",tp\n",
    "print \"FP: \",fp\n",
    "print \"TN: \",tn\n",
    "print \"FN: \",fn\n",
    "print \"Precision: \",precision\n",
    "print \"Recall: \",recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
